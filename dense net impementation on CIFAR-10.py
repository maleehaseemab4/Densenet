# -*- coding: utf-8 -*-
"""densenet simple.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-B-6K9KWz8vcu6_qSrHIX7-Sn6RnYRs
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.models import densenet121
import torch.nn as nn
import torch.optim as optim

dataset_choice = 'cifar10'

transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

if dataset_choice == 'cifar10':
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    num_classes = 10
else:  # cifar100
    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)
    num_classes = 100

trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)

model = densenet121(pretrained=True)

model.classifier = nn.Linear(model.classifier.in_features, num_classes)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

import time
import matplotlib.pyplot as plt
import numpy as np

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

model = model.to(device)

num_epochs = 5
print(f"Training DenseNet on {dataset_choice} for {num_epochs} epochs")

# Lists to track progress
train_losses = []
accuracies = []

import time
# Training loop
for epoch in range(num_epochs):
    start_time = time.time()
    running_loss = 0.0

    # Training phase
    model.train()
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()
        if i % 100 == 99:    # Print every 100 mini-batches
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            train_losses.append(running_loss / 100)
            running_loss = 0.0

# Validation phase
model.eval()
correct = 0
total = 0
start_time = time.time()
with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

epoch_accuracy = 100 * correct / total
accuracies.append(epoch_accuracy)

epoch_time = time.time() - start_time
print(f'Epoch {epoch + 1} completed in {epoch_time:.2f}s | Accuracy: {epoch_accuracy:.2f}%')

print('Finished Training')

torch.save(model.state_dict(), f'densenet_{dataset_choice}.pth')

# Plot training progress
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_losses)
plt.title('Training Loss')
plt.xlabel('Mini-batches (x100)')
plt.ylabel('Loss')

plt.subplot(1, 2, 2)
plt.plot(accuracies)
plt.title('Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.tight_layout()
plt.show()

# Visualize some predictions
model.eval()
dataiter = iter(testloader)
images, labels = next(dataiter)
images = images.to(device)
labels = labels.to(device)

# Get predictions
outputs = model(images)
_, predicted = torch.max(outputs, 1)

# Get class names
if dataset_choice == 'cifar10':
    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')
else:  # CIFAR-100 has too many classes to display, so we'll just use indices
    classes = [str(i) for i in range(100)]

# Plot some images with predictions
plt.figure(figsize=(12, 8))
for i in range(min(8, len(images))):
    plt.subplot(2, 4, i+1)
    # Convert tensor back to image format for display
    img = images[i].cpu().numpy().transpose((1, 2, 0))
    # Undo normalization for display
    img = img * 0.5 + 0.5
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    plt.title(f'Pred: {classes[predicted[i]]}\nTrue: {classes[labels[i]]}')
    plt.axis('off')
plt.tight_layout()
plt.show()

# Print overall model performance
correct = 0
total = 0
class_correct = list(0. for i in range(num_classes))
class_total = list(0. for i in range(num_classes))

with torch.no_grad():
    for data in testloader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()

        for i in range(len(labels)):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Overall accuracy on test set: {100 * correct / total:.2f}%')

show_classes = min(10, num_classes)
for i in range(show_classes):
    class_name = classes[i]
    if class_total[i] > 0:
        accuracy = 100 * class_correct[i] / class_total[i]
        print(f'Accuracy of {class_name}: {accuracy:.2f}%')